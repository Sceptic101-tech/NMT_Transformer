{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.custom_dataset import CustomDataset\n",
    "from scripts.vectorizer import Seq2Seq_Vectorizer\n",
    "from scripts.tokenizer import SeparatorTokenizer\n",
    "from scripts.vocabulary import Vocabulary\n",
    "from scripts.model import TransformerModel, subsequent_mask\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745196c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROPORTION = 0.0\n",
    "EVAL_PROPORTION = 0.2\n",
    "\n",
    "TOKENS_TRESHOLD_FREQ = 10\n",
    "\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "LR_SCHEDULER_PATIENCE = 2\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 128\n",
    "MODEL_DIM = 256\n",
    "NUM_HEAD = 6\n",
    "NUM_ENCODER_LAYERS = 5\n",
    "NUM_DECODER_LAYERS = 5\n",
    "FC_HIDDEN_DIM = MODEL_DIM*4\n",
    "DROPOUT = 0.1\n",
    "TEMPERATURE = 0.9\n",
    "BATCH_FIRST = True\n",
    "\n",
    "MAX_SOURCE_SEQ_LEN = 100\n",
    "MAX_TARGET_SEQ_LEN = 114\n",
    "MAX_SEQ_LEN = 200\n",
    "\n",
    "MODEL_SAVE_FILEPATH = 'data/model_params.pt'\n",
    "DATASET_PATH = 'D:/Files/Datasets/NMT_ru_en'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32492a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''Перенос данных на device и подготовка к упаковке в padded_seq'''\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"Normalize tensor sizes\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): the output of the model\n",
    "            If a 3-dimensional tensor, reshapes to a matrix\n",
    "        y_true (torch.Tensor): the target predictions\n",
    "            If a matrix, reshapes to be a vector\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.reshape(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.reshape(-1)\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21469582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_freq(dataframe : pandas.DataFrame) -> tuple[dict, dict]:\n",
    "    '''Принимает токенизированный датафрейм'''\n",
    "    source_freq = {}\n",
    "    target_freq = {}\n",
    "    for i in range(len(dataframe)):\n",
    "        source_tokens, target_tokens = (dataframe.loc[i, 'source_text'], dataframe.loc[i, 'target_text'])\n",
    "        for token in source_tokens:\n",
    "            if token in source_freq:\n",
    "                source_freq[token] += 1\n",
    "            else:\n",
    "                source_freq[token] = 1\n",
    "        for token in target_tokens:\n",
    "            if token in target_freq:\n",
    "                target_freq[token] += 1\n",
    "            else:\n",
    "                target_freq[token] = 1\n",
    "    return source_freq, target_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8912167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokenized_seq_len(dataframe : pandas.DataFrame) -> tuple[int, int]:\n",
    "    '''Принимает датафрейм с токенизированными предложениями.\n",
    "    Возвращает два списка: длина исходных текстов, длина таргет текстов'''\n",
    "    source_max_len = target_max_len = -1\n",
    "    for idx in range(len(dataframe)):\n",
    "        source_max_len = max(len(dataframe.loc[idx, 'source_text']), source_max_len)\n",
    "        target_max_len = max(len(dataframe.loc[idx, 'target_text']), target_max_len)\n",
    "    return source_max_len, target_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca05fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, vectorizer, query : str, max_seq_len : int = 100, temperature : float = 1.0, device : str = 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    bos_index = vectorizer.target_vocab._bos_index\n",
    "    source_mask_index = vectorizer.source_vocab.mask_token_index\n",
    "    target_mask_index = vectorizer.target_vocab.mask_token_index\n",
    "\n",
    "    tokenized_lst = tokenizer.tokenize(query)\n",
    "    vectorized_dict = vectorizer.vectorize(source_tokens=tokenized_lst, use_dataset_max_len=False)\n",
    "\n",
    "    # pass through encoder\n",
    "    source = torch.tensor(vectorized_dict['source_vec'], dtype=torch.long).to(device).unsqueeze(0)\n",
    "    embeded = model.source_embedding(source) * math.sqrt(model.embed_dim)\n",
    "    pos_embeded = model.pos_encoding_encoder(embeded)\n",
    "    source_embed_projected = model.embed_to_model_projection(pos_embeded)\n",
    "\n",
    "    source_key_padding_mask = (source == source_mask_index).to(device)\n",
    "\n",
    "    encoder_output = model.transformer.encoder(source_embed_projected, src_key_padding_mask=source_key_padding_mask)\n",
    "\n",
    "    # Инициализация decoder input с <BOS> токеном\n",
    "    decoder_input = torch.tensor([[vectorizer.target_vocab._bos_index]], \n",
    "                                device=device, dtype=torch.long)\n",
    "        \n",
    "    # Пошаговая генерация последовательности\n",
    "    for _ in range(max_seq_len):\n",
    "        # Forward pass через decoder\n",
    "        target_embed = model.target_embedding(decoder_input) * math.sqrt(model.embed_dim)\n",
    "        target_embed = model.pos_encoding_decoder(target_embed)\n",
    "        target_embed_projected = model.embed_to_model_projection(target_embed)\n",
    "            \n",
    "        decoder_output = model.transformer.decoder(\n",
    "            target_embed_projected,\n",
    "            encoder_output,\n",
    "            tgt_mask=subsequent_mask(decoder_input.size(1), device=device),\n",
    "            tgt_key_padding_mask=(decoder_input == target_mask_index),\n",
    "            memory_key_padding_mask=source_key_padding_mask)\n",
    "\n",
    "        # Получение предсказания следующего токена\n",
    "        logits = model.classifier(decoder_output[:, -1, :])\n",
    "        probs = F.softmax(logits/temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "        # Добавление нового токена к последовательности\n",
    "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "            \n",
    "        # Проверка на окончание последовательности\n",
    "        if next_token.item() == vectorizer.target_vocab._eos_index:\n",
    "            return decoder_input\n",
    "            break\n",
    "\n",
    "    # Декодирование индексов в текст\n",
    "    return decoder_input\n",
    "    # return vectorizer.target_vocab.get_token(next_token.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49240e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_indices(indices : torch.tensor, vectorizer):\n",
    "    seq_count, seq_len = (indices.size(0), indices.size(1))\n",
    "    vocab = vectorizer.target_vocab\n",
    "    decoded = []\n",
    "    for seq in range(seq_count):\n",
    "        string =''\n",
    "        for idx in range(seq_len):\n",
    "            index = indices[seq, idx].item()\n",
    "            if index != vocab.mask_token_index:\n",
    "                string += vocab.get_token(index) + ' '\n",
    "            if index == vocab._eos_index:\n",
    "                break\n",
    "        decoded.append(string)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_file(model, filepath):\n",
    "    torch.save(model, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6852d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SeparatorTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094924f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATASET_PATH, 'ru_en_small.csv'), index_col='Unnamed: 0')\n",
    "\n",
    "df = df.rename(columns={'ru_text' : 'source_text', 'en_text' : 'target_text'})\n",
    "df['split'] = 'train'\n",
    "selected_indices = df.sample(int(EVAL_PROPORTION*len(df)), random_state=RANDOM_STATE).index\n",
    "df.loc[selected_indices, 'split'] = 'validation'\n",
    "\n",
    "# К нижнему регистру, токенизация и очистка от служебных символов\n",
    "df['source_text'] = df['source_text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df['target_text'] = df['target_text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск максимальной длины сурс и таргет текста\n",
    "# source_max_len = target_max_len = -1\n",
    "# for i in range(len(df)):\n",
    "#     source_max_len = max(source_max_len, len(df.loc[i, 'source_text']))\n",
    "#     target_max_len = max(target_max_len, len(df.loc[i, 'target_text']))\n",
    "# print(source_max_len)\n",
    "# print(target_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb99cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = Vocabulary()\n",
    "target_vocab = Vocabulary()\n",
    "\n",
    "source_freq, target_freq = get_tokens_freq(df)\n",
    "\n",
    "for key, value in source_freq.items():\n",
    "    if value > TOKENS_TRESHOLD_FREQ:\n",
    "        source_vocab.add_token(key)\n",
    "\n",
    "for key, value in target_freq.items():\n",
    "    if value > TOKENS_TRESHOLD_FREQ:\n",
    "        target_vocab.add_token(key)\n",
    "\n",
    "\n",
    "source_vocab.to_json('data/source_vocab.json')\n",
    "target_vocab.to_json('data/target_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a81e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = Vocabulary.from_json('data/source_vocab.json')\n",
    "target_vocab = Vocabulary.from_json('data/target_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Seq2Seq_Vectorizer(source_vocab, target_vocab, MAX_SOURCE_SEQ_LEN, MAX_TARGET_SEQ_LEN)\n",
    "dataset = CustomDataset(df, tokenizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_size = len(source_vocab)\n",
    "target_vocab_size = len(target_vocab)\n",
    "mask_index = target_vocab.mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7091261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(source_vocab_size, target_vocab_size, EMBEDDING_DIM, MODEL_DIM, NUM_HEAD, NUM_ENCODER_LAYERS,\\\n",
    "#                          NUM_DECODER_LAYERS, FC_HIDDEN_DIM, DROPOUT, MAX_SEQ_LEN, BATCH_FIRST, mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_SAVE_FILEPATH, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62366ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        dataset.set_dataframe_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, drop_last=DROP_LAST, device=DEVICE)\n",
    "        train_running_loss = 0.0\n",
    "        train_running_acc = 0.0\n",
    "        epoch_err = 0.0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute the output\n",
    "            y_pred = model(batch_dict['source_vec'],\n",
    "                           batch_dict['target_x_vec'],\n",
    "                           apply_softmax = False,\n",
    "                           temperature = TEMPERATURE)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['target_y_vec'], mask_index=mask_index)\n",
    "\n",
    "            # use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute the running loss and running accuracy\n",
    "            train_running_loss += (loss.item() - train_running_loss) / (batch_index + 1)\n",
    "            epoch_err += loss.item()\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "            train_running_acc += (acc_t - train_running_acc) / (batch_index + 1)\n",
    "\n",
    "        print('-'*40)\n",
    "        print(f'epoch {epoch+1}')\n",
    "        print(f'train_epoch_error {epoch_err}')\n",
    "        print(f'train loss {train_running_loss}   ,   train accuracy {train_running_acc}')\n",
    "\n",
    "\n",
    "        dataset.set_dataframe_split('validation')\n",
    "        batch_generator = generate_batches(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, drop_last=DROP_LAST, device=DEVICE)\n",
    "        valid_running_loss = 0.0\n",
    "        valid_running_acc = 0.0\n",
    "        epoch_err = 0.0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(batch_dict['source_vec'],\n",
    "                           batch_dict['target_x_vec'],\n",
    "                           apply_softmax = False,\n",
    "                           temperature = TEMPERATURE)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "\n",
    "            # compute the running loss and accuracy\n",
    "            valid_running_loss += (loss.item() - valid_running_loss) / (batch_index + 1)\n",
    "            epoch_err += loss.item()\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "            valid_running_acc += (acc_t - valid_running_acc) / (batch_index + 1)\n",
    "\n",
    "        print(f'validation_epoch_error {epoch_err}')\n",
    "        print(f'validation loss {valid_running_loss}   ,   validation accuracy {valid_running_acc}')\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1de075",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Я ем яблоко'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = generate(model, tokenizer, vectorizer, query, max_seq_len=MAX_SEQ_LEN, temperature=TEMPERATURE, device=DEVICE)\n",
    "response = decode_indices(indices, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b953a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_to_file(model, MODEL_SAVE_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6945d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
