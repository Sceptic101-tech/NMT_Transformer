{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d126e0",
   "metadata": {},
   "source": [
    "При обучении >15 эпох, качество ответов модели падает, все ответы становятся однообразными.\\\n",
    "\n",
    "TODO Увеличить количество ингредиентов для каждого рецепта. В данным момент - расчет на порцию => Генерируемые рецепты содержат слишком малое их количество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e998619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.custom_dataset import CustomDataset\n",
    "from scripts.vectorizer import Seq2Seq_Vectorizer\n",
    "from scripts.tokenizer import SeparatorTokenizer\n",
    "from scripts.vocabulary import Vocabulary\n",
    "from scripts.model import TransformerModel, subsequent_mask\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745196c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROPORTION = 0.0\n",
    "EVAL_PROPORTION = 0.0\n",
    "\n",
    "TOKENS_TRESHOLD_FREQ = 10\n",
    "\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "EPOCHS = 0\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "LR_SCHEDULER_PATIENCE = 2\n",
    "\n",
    "USE_PRETRAINED = True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "MODEL_DIM = 384\n",
    "NUM_HEAD = 8\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 6\n",
    "FC_HIDDEN_DIM = MODEL_DIM*4 # Как в классическом трансформере\n",
    "DROPOUT = 0.1\n",
    "TEMPERATURE = 0.7\n",
    "BATCH_FIRST = True\n",
    "\n",
    "MAX_SOURCE_SEQ_LEN = 30 # Максимальная длина берется из датафрейма\n",
    "MAX_TARGET_SEQ_LEN = 1300\n",
    "MAX_SEQ_LEN = max(MAX_SOURCE_SEQ_LEN, MAX_TARGET_SEQ_LEN)\n",
    "\n",
    "MODEL_SAVE_FILEPATH = 'data/model_params_recipes.pt'\n",
    "DATASET_FILEPATH = 'D:/Files/Datasets/recipes_generation/all_recepies_preprocessed.csv'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d8df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32492a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    Создает батчи из датасета и переносит данные на указанное устройство.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Датасет для создания батчей\n",
    "        batch_size (int): Размер батча\n",
    "        shuffle (bool): Перемешивать ли данные\n",
    "        drop_last (bool): Отбрасывать ли последний неполный батч\n",
    "        device (str): Устройство для переноса данных ('cpu' или 'cuda')\n",
    "    \n",
    "    Yields:\n",
    "        dict: Словарь с тензорами батча, перенесенными на устройство\n",
    "    '''\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918b42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred:torch.tensor, y_true:torch.tensor):\n",
    "    '''\n",
    "    Нормализует размеры тензоров предсказаний и целевых значений.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): Тензор предсказаний модели (3D или 2D)\n",
    "        y_true (torch.Tensor): Тензор целевых значений (2D или 1D)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Нормализованные тензоры (y_pred, y_true)\n",
    "    '''\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.reshape(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.reshape(-1)\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59a20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    '''\n",
    "    Вычисляет точность предсказаний модели, игнорируя маскированные токены.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): Тензор предсказаний модели\n",
    "        y_true (torch.Tensor): Тензор целевых значений\n",
    "        mask_index (int): Индекс маскированного токена\n",
    "    \n",
    "    Returns:\n",
    "        float: Значение точности в процентах\n",
    "    '''\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21469582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    '''\n",
    "    Вычисляет функцию потерь для последовательностей с игнорированием маскированных токенов.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): Тензор предсказаний модели\n",
    "        y_true (torch.Tensor): Тензор целевых значений\n",
    "        mask_index (int): Индекс маскированного токена\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Значение функции потерь\n",
    "    '''\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a1838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_freq(dataframe: pandas.DataFrame) -> tuple[dict, dict]:\n",
    "    '''\n",
    "    Вычисляет частоту встречаемости токенов в датафрейме.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Датафрейм с токенизированными текстами\n",
    "    \n",
    "    Returns:\n",
    "        dict: Словарь с частотой токенов\n",
    "    '''\n",
    "    tokens_freq = {}\n",
    "    for i in range(len(dataframe)):\n",
    "        source_tokens, target_tokens = (dataframe.loc[i, 'source_text'], dataframe.loc[i, 'target_text'])\n",
    "        for token in source_tokens:\n",
    "            if token in tokens_freq:\n",
    "                tokens_freq[token] += 1\n",
    "            else:\n",
    "                tokens_freq[token] = 1\n",
    "        for token in target_tokens:\n",
    "            if token in tokens_freq:\n",
    "                tokens_freq[token] += 1\n",
    "            else:\n",
    "                tokens_freq[token] = 1\n",
    "    return tokens_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8912167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokenized_seq_len(dataframe: pandas.DataFrame) -> tuple[int, int]:\n",
    "    '''\n",
    "    Находит максимальную длину исходных и целевых последовательностей в датафрейме.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Датафрейм с токенизированными текстами\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (макс. длина исходных текстов, макс. длина целевых текстов)\n",
    "    '''\n",
    "    source_max_len = target_max_len = -1\n",
    "    for idx in range(len(dataframe)):\n",
    "        source_max_len = max(len(dataframe.loc[idx, 'source_text']), source_max_len)\n",
    "        target_max_len = max(len(dataframe.loc[idx, 'target_text']), target_max_len)\n",
    "    return source_max_len, target_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca05fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO доделать генерацию нескольких ответов за запуск\n",
    "def generate(model, tokenizer, vectorizer, query: str, max_seq_len: int = 100, \n",
    "             seq_cnt: int = 1, temperature: float = 1.0, device: str = 'cpu'):\n",
    "    '''\n",
    "    Генерирует последовательность на основе входного запроса с помощью модели.\n",
    "    \n",
    "    Args:\n",
    "        model (TransformerModel): Обученная модель трансформера\n",
    "        tokenizer (SeparatorTokenizer): Токенизатор для обработки текста\n",
    "        vectorizer (Seq2Seq_Vectorizer): Векторизатор для преобразования токенов\n",
    "        query (str): Входной запрос для генерации\n",
    "        max_seq_len (int): Максимальная длина генерируемой последовательности\n",
    "        seq_cnt (int): Количество генерируемых последовательностей\n",
    "        temperature (float): Температура для генерации (контроль случайности)\n",
    "        device (str): Устройство для вычислений\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Тензор с сгенерированными последовательностями индексов токенов\n",
    "    '''\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    bos_index = vectorizer.tokens_vocab._bos_index\n",
    "    eos_index = vectorizer.tokens_vocab._eos_index\n",
    "    source_mask_index = vectorizer.tokens_vocab.mask_token_index\n",
    "    target_mask_index = vectorizer.tokens_vocab.mask_token_index\n",
    "\n",
    "    tokenized_lst = tokenizer.tokenize(query)\n",
    "    vectorized_dict = vectorizer.vectorize(source_tokens=tokenized_lst, use_dataset_max_len=False)\n",
    "\n",
    "    source = torch.tensor(vectorized_dict['source_vec'], dtype=torch.int).to(device).unsqueeze(0)\n",
    "    source = source.expand(seq_cnt, -1) # Расширяем для генерации нескольких ответов\n",
    "\n",
    "    embeded = model.source_embedding(source)\n",
    "    pos_embeded = model.pos_encoding_encoder(embeded) * math.sqrt(model.embed_dim)\n",
    "    source_embed_projected = model.embed_to_model_projection(pos_embeded)\n",
    "    source_key_padding_mask = (source == source_mask_index).to(device)\n",
    "\n",
    "    # Проход через энкодер\n",
    "    encoder_output = model.transformer.encoder(source_embed_projected, src_key_padding_mask=source_key_padding_mask)\n",
    "\n",
    "    # Инициализация decoder_input <BOS> токеном\n",
    "    bos_list = [[bos_index]] * seq_cnt\n",
    "    decoder_input = torch.tensor(bos_list, device=device, dtype=torch.int)\n",
    "        \n",
    "    # Пошаговая генерация последовательности\n",
    "    for _ in range(max_seq_len):\n",
    "        # Проход через декодер\n",
    "\n",
    "        target_embed = model.target_embedding(decoder_input)\n",
    "        target_embed = model.pos_encoding_decoder(target_embed) * math.sqrt(model.embed_dim)\n",
    "        target_embed_projected = model.embed_to_model_projection(target_embed)\n",
    "            \n",
    "        decoder_output = model.transformer.decoder(\n",
    "            target_embed_projected,\n",
    "            encoder_output,\n",
    "            tgt_mask=subsequent_mask(decoder_input.size(1), device=device),\n",
    "            tgt_key_padding_mask=(decoder_input == target_mask_index),\n",
    "            memory_key_padding_mask=source_key_padding_mask)\n",
    "\n",
    "        # Получение предсказания следующего токена\n",
    "        logits = model.classifier(decoder_output[:, -1, :])\n",
    "        probs = F.softmax(logits/temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "        # Добавление нового токена к последовательности\n",
    "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "            \n",
    "        # Проверка на окончание последовательности\n",
    "        if next_token.size(0) == 1:\n",
    "            if next_token.item() == eos_index:\n",
    "                return decoder_input\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49240e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_indices(indices: torch.tensor, vectorizer):\n",
    "    '''\n",
    "    Декодирует тензор индексов обратно в текстовое представление.\n",
    "    \n",
    "    Args:\n",
    "        indices (torch.Tensor): Тензор с индексами токенов\n",
    "        vectorizer (Seq2Seq_Vectorizer): Векторизатор для обратного преобразования\n",
    "    \n",
    "    Returns:\n",
    "        list: Список декодированных строк\n",
    "    '''\n",
    "    seq_count, seq_len = (indices.size(0), indices.size(1))\n",
    "    vocab = vectorizer.tokens_vocab\n",
    "    decoded = []\n",
    "    for seq in range(seq_count):\n",
    "        string =''\n",
    "        for idx in range(seq_len):\n",
    "            index = indices[seq, idx].item()\n",
    "            if index != vocab.mask_token_index:\n",
    "                string += vocab.get_token(index) + ' '\n",
    "            if index == vocab._eos_index:\n",
    "                break\n",
    "        decoded.append(string)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb1c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(model, model_filepath:str, train_states:list, validation_states:list):\n",
    "    '''\n",
    "    Сохраняет параметры модели и метрики обучения в файлы.\n",
    "    \n",
    "    Args:\n",
    "        model (TransformerModel): Обученная модель\n",
    "        model_filepath (str): Путь для сохранения модели\n",
    "        train_states (list): Метрики обучения\n",
    "        validation_states (list): Метрики валидации\n",
    "    '''\n",
    "    torch.save(model, model_filepath)\n",
    "    with open(\"data/train_states.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(train_states, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    with open(\"data/validation_states.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(validation_states, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ec304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_source_target_seq_len(df:pandas.DataFrame):\n",
    "    source_max_len = target_max_len = -1\n",
    "    for i in range(len(df)):\n",
    "        source_max_len = max(source_max_len, len(df.loc[i, 'source_text']))\n",
    "        target_max_len = max(target_max_len, len(df.loc[i, 'target_text']))\n",
    "    return (source_max_len, target_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6852d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SeparatorTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c919c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4751f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_text'] = ''\n",
    "df = df.rename(columns={'name' : 'source_text'})\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, 'ingredients'] = re.sub(r'\\. ?\\.\\n', '.\\n ', df.loc[i, 'ingredients'])\n",
    "    df.loc[i, 'target_text'] = f'НАЗВАНИЕ: {df.loc[i, 'source_text']}.\\n ИНГРЕДИЕНТЫ: {df.loc[i, 'ingredients']}.\\n ИНСТРУКЦИЯ: {df.loc[i, 'Instructions']}.'\n",
    "df = df.drop(columns=['Instructions', 'ingredients', 'composition_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "094924f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = 'train'\n",
    "selected_indices = df.sample(int(EVAL_PROPORTION*len(df)), random_state=RANDOM_STATE).index\n",
    "df.loc[selected_indices, 'split'] = 'validation'\n",
    "\n",
    "# К нижнему регистру, токенизация и очистка от служебных символов\n",
    "df['source_text'] = df['source_text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df['target_text'] = df['target_text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e033ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "1273\n"
     ]
    }
   ],
   "source": [
    "source_max_len, target_max_len = get_max_source_target_seq_len(df)\n",
    "print(source_max_len)\n",
    "print(target_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfb99cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocab = Vocabulary()\n",
    "\n",
    "tokens_freq = get_tokens_freq(df)\n",
    "\n",
    "for key, value in tokens_freq.items():\n",
    "    if value > TOKENS_TRESHOLD_FREQ:\n",
    "        tokens_vocab.add_token(key)\n",
    "\n",
    "tokens_vocab.to_json('data/tokens_vocab_recipes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a81e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocab = Vocabulary.from_json('data/tokens_vocab_recipes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b390ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Seq2Seq_Vectorizer(tokens_vocab, MAX_SOURCE_SEQ_LEN, MAX_TARGET_SEQ_LEN)\n",
    "dataset = CustomDataset(df, tokenizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c640f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokens_vocab)\n",
    "mask_index = tokens_vocab.mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "722106b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED:\n",
    "    with open(\"data/train_states.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        train_states = json.load(file)\n",
    "\n",
    "    with open(\"data/validation_states.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        validation_states = json.load(file)\n",
    "    \n",
    "    model = torch.load(MODEL_SAVE_FILEPATH, weights_only=False)\n",
    "else:\n",
    "    train_states = []\n",
    "    validation_states = []\n",
    "    model = TransformerModel(vocab_size, vocab_size, EMBEDDING_DIM, MODEL_DIM, NUM_HEAD, NUM_ENCODER_LAYERS,\\\n",
    "                         NUM_DECODER_LAYERS, FC_HIDDEN_DIM, DROPOUT, MAX_SEQ_LEN, BATCH_FIRST, mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62366ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        dataset.set_dataframe_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, drop_last=DROP_LAST, device=DEVICE)\n",
    "        train_running_loss = 0.0\n",
    "        train_running_acc = 0.0\n",
    "        epoch_err = 0.0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Предсказание\n",
    "            y_pred = model(batch_dict['source_vec'],\n",
    "                           batch_dict['target_x_vec'],\n",
    "                           apply_softmax = False,\n",
    "                           temperature = TEMPERATURE)\n",
    "\n",
    "            # потери\n",
    "            loss = sequence_loss(y_pred, batch_dict['target_y_vec'], mask_index=mask_index)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Средние потери и точность\n",
    "            train_running_loss += (loss.item() - train_running_loss) / (batch_index + 1)\n",
    "            epoch_err += loss.item()\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "            train_running_acc += (acc_t - train_running_acc) / (batch_index + 1)\n",
    "\n",
    "        train_states.append({'epoch' : epoch+1, 'epoch_loss' : epoch_err, 'epoch_running_loss' : train_running_loss, 'accuracy' : train_running_acc})\n",
    "\n",
    "        print('-'*40)\n",
    "        print(f'epoch {epoch+1}')\n",
    "        print(f'train_epoch_error {epoch_err}')\n",
    "        print(f'train loss {train_running_loss}   ,   train accuracy {train_running_acc}')\n",
    "\n",
    "\n",
    "        if EVAL_PROPORTION > 0:\n",
    "            dataset.set_dataframe_split('validation')\n",
    "            batch_generator = generate_batches(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, drop_last=DROP_LAST, device=DEVICE)\n",
    "            valid_running_loss = 0.0\n",
    "            valid_running_acc = 0.0\n",
    "            epoch_err = 0.0\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                    # Предсказание\n",
    "                    y_pred = model(batch_dict['source_vec'],\n",
    "                                batch_dict['target_x_vec'],\n",
    "                                apply_softmax = False,\n",
    "                                temperature = TEMPERATURE)\n",
    "\n",
    "                    # потери\n",
    "                    loss = sequence_loss(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "\n",
    "                    # Средние потери и точность\n",
    "                    valid_running_loss += (loss.item() - valid_running_loss) / (batch_index + 1)\n",
    "                    epoch_err += loss.item()\n",
    "\n",
    "                    acc_t = compute_accuracy(y_pred, batch_dict['target_y_vec'], mask_index)\n",
    "                    valid_running_acc += (acc_t - valid_running_acc) / (batch_index + 1)\n",
    "\n",
    "            validation_states.append({'epoch' : epoch+1, 'epoch_loss' : epoch_err, 'epoch_running_loss' : valid_running_loss, 'accuracy' : valid_running_acc})\n",
    "\n",
    "            scheduler.step(valid_running_loss)\n",
    "\n",
    "            print(f'validation_epoch_error {epoch_err}')\n",
    "            print(f'validation loss {valid_running_loss}   ,   validation accuracy {valid_running_acc}')\n",
    "        else:\n",
    "            scheduler.step(train_running_loss)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Аварийная остановка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a1de075",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'пончики с маслом'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dccbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.lower()\n",
    "indices = generate(model, tokenizer, vectorizer, query, max_seq_len=MAX_SEQ_LEN, temperature=TEMPERATURE, device=DEVICE)\n",
    "response = decode_indices(indices, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b953a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> название : творожные с вишней . ингредиенты : пшеничная мука 0 . 5 стак . вода 0 . 2 стак . сливочное масло 0 . 5 столов . яйцо куриное 0 . 5 шт . молоко 0 . 2 стак . сахар 0 . 2 стак . соль щепотка по_вкусу . . инструкция : 1 . смешать муку , соль , разрыхлитель и соль . 2 . в глубокой миске взбить яйца , всыпать муку и сахар . 3 . добавить молоко , пока не станет однородной . 4 . добавить в тесто лук и быстро вымесить тесто . 5 . выпекать в течение 30 - 40 минут . 6 . выложить на бумажное полотенце . . <EOS> ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f214eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_to_file(model, MODEL_SAVE_FILEPATH, train_states, validation_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeef39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
